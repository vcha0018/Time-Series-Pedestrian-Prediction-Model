{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Data processing for Big data\n",
    "\n",
    "##  Activity: Assignment 2 Part B\n",
    "\n",
    "### Task 2\n",
    "\n",
    "##### Student ID: `31265154`\n",
    "##### Student Name: `Vivekkumar Chaudhari`\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "* [1. Spark Structured Streaming with Spark DataStreams](#one)\n",
    "    * [1.1 Initialization of SparkSession](#oneone)\n",
    "    * [1.2 Define the data schema for the sensor location CSV](#onetwo)\n",
    "    * [1.3 Ingest the streaming data into Spark Streaming](#onethree)\n",
    "    * [1.4 Persist the raw streaming data in parquet format](#onefour)\n",
    "    * [1.5 Transform the streaming data into the proper formats](#onefive)\n",
    "    * [1.6 Transformations to prepare the columns for model prediction](#onesix)\n",
    "    * [1.7 Derive model predictions and saving it into parquet format](#oneseven)\n",
    "    * [1.8 (a) Counting above throshold Hourly counts from predicted result](#oneeighta)\n",
    "    * [1.8 (b) Getting above threshold for next day from prediction result and write to new Kafka topic](#oneeightb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"one\"></a>\n",
    "# 1. Spark Structured Streaming with Spark DataStreams\n",
    "<a class=\"anchor\" name=\"oneone\"></a>\n",
    "## 1.1 Initialization of SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import feature as FT\n",
    "from pyspark.ml import classification as C\n",
    "from pyspark.ml import regression as R\n",
    "from pyspark.ml import evaluation as E\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "from time import sleep\n",
    "import geohash\n",
    "\n",
    "## Creating Spark Configuration\n",
    "# Use all available cores\n",
    "master = 'local[*]'\n",
    "# Spark Application name\n",
    "app_name = 'Streaming Pedestrian Predictions'\n",
    "# Creating spark session object with default UTC timezone. \n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    config(conf=SparkConf().\\\n",
    "           setMaster(master).\\\n",
    "           setAppName(app_name).\\\n",
    "           set(\"spark.sql.session.timeZone\", \"UTC\")).\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"onetwo\"></a>\n",
    "## 1.2 Define the data schema for the sensor location CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- sensor_description: string (nullable = true)\n",
      " |-- sensor_name: string (nullable = true)\n",
      " |-- installation_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- direction_1: string (nullable = true)\n",
      " |-- direction_2: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.2\n",
    "# defining the schema of sensor location dataframe\n",
    "sensor_location_schema = T.StructType([\n",
    "    T.StructField('sensor_id', T.IntegerType(), True),\n",
    "    T.StructField('sensor_description', T.StringType(), True),\n",
    "    T.StructField('sensor_name', T.StringType(), True),\n",
    "    T.StructField('installation_date', T.DateType(), True),\n",
    "    T.StructField('status', T.StringType(), True),\n",
    "    T.StructField('note', T.StringType(), True),\n",
    "    T.StructField('direction_1', T.StringType(), True),\n",
    "    T.StructField('direction_2', T.StringType(), True),\n",
    "    T.StructField('latitude', T.FloatType(), True),\n",
    "    T.StructField('longitude', T.FloatType(), True),\n",
    "    T.StructField('location', T.StringType(), True),\n",
    "])\n",
    "\n",
    "# reading sensor locations csv file to dataframe with header and schema.\n",
    "df_sensors = spark.read.\\\n",
    "            format('csv').\\\n",
    "            option('header', 'true').\\\n",
    "            option('escape', '\"').\\\n",
    "            option(\"dateFormat\", 'yyyy/MM/dd').\\\n",
    "            schema(sensor_location_schema).\\\n",
    "            load('Pedestrian_Counting_System_-_Sensor_Locations.csv')\n",
    "\n",
    "# print the schema of sensor location csv dataframe\n",
    "df_sensors.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"onethree\"></a>\n",
    "## 1.3 Ingest the streaming data into Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- day_data: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3\n",
    "# Create the same topic as in Task 1.\n",
    "topic = 'Pedestrian_data'\n",
    "# Create the spark's read stream at same location and port as producer\n",
    "# And set starting offeset of reading stream to earliest to consume all available data on the topic from start.\n",
    "# In case of reading stream starts late.\n",
    "pedestrian_stream = spark.\\\n",
    "    readStream.\\\n",
    "    format(\"kafka\").\\\n",
    "    option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\").\\\n",
    "    option(\"subscribe\", topic).\\\n",
    "    option(\"startingOffsets\", \"earliest\").\\\n",
    "    load()\n",
    "\n",
    "pedestrian_stream.printSchema()\n",
    "# Convert byte value to string data\n",
    "df_pedestrian = pedestrian_stream.selectExpr(\"CAST(value AS STRING) AS day_data\", 'timestamp')\n",
    "df_pedestrian.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"onefour\"></a>\n",
    "## 1.4 Persist the raw streaming data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f7293e87130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.4\n",
    "# Write the raw data of read stream to file sink in parquet format\n",
    "# Writing every new avaialble data with append outputmode.\n",
    "# With the same trigger as in Task 1. (daywise)\n",
    "pedestrian_stream.\\\n",
    "    writeStream.\\\n",
    "    format('parquet').\\\n",
    "    outputMode(\"append\").\\\n",
    "    option(\"path\", \"parquet/raw_data_stream.parquet\").\\\n",
    "    option(\"checkpointLocation\", \"parquet/raw_data_stream_checkpoint\").\\\n",
    "    trigger(processingTime='5 seconds').\\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"onefive\"></a>\n",
    "## 1.5 Transform the streaming data into the proper formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.5\n",
    "# defining the schema of monthly ped. count dataframe\n",
    "count_schema = T.ArrayType( \n",
    "    T.StructType([\n",
    "        T.StructField('ID', T.IntegerType(), True),\n",
    "        T.StructField('Date_Time', T.StringType(), True),\n",
    "        T.StructField('Year', T.IntegerType(), True),\n",
    "        T.StructField('Month', T.StringType(), True),\n",
    "        T.StructField('Mdate', T.IntegerType(), True),\n",
    "        T.StructField('Day', T.StringType(), True),\n",
    "        T.StructField('Time', T.IntegerType(), True),\n",
    "        T.StructField('Sensor_ID', T.IntegerType(), True),\n",
    "        T.StructField('Sensor_Name', T.StringType(), True),\n",
    "        T.StructField('Hourly_Counts', T.IntegerType(), True)\n",
    "    ]))\n",
    "\n",
    "# Convert string data to required format using schema format. \n",
    "df_counts = df_pedestrian.\\\n",
    "    select(\n",
    "        F.from_json(F.col(\"day_data\").cast(\"string\"), count_schema).alias('parsed_value'))\n",
    "# Flatterns nested data \n",
    "df_counts = df_counts.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n",
    "# Rename the coulumns and convert date time column type format.\n",
    "df_counts = df_counts.\\\n",
    "    filter(F.col('unnested_value.Time').between(9, 23)).\\\n",
    "    withColumn(\"Date_Time\", F.to_timestamp(F.col('unnested_value.Date_Time'), 'MM/dd/yyyy hh:mm:ss a').cast('timestamp')).\\\n",
    "    select(\n",
    "        F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
    "        'Date_Time',\n",
    "        F.col(\"unnested_value.Year\").alias(\"Year\"),\n",
    "        F.col(\"unnested_value.Month\").alias(\"Month\"),\n",
    "        F.col(\"unnested_value.Mdate\").alias(\"Mdate\"),\n",
    "        F.col(\"unnested_value.Day\").alias(\"Day\"),\n",
    "        F.col(\"unnested_value.Time\").alias(\"Time\"),\n",
    "        F.col(\"unnested_value.Sensor_ID\").alias(\"Sensor_ID\"),\n",
    "        F.col(\"unnested_value.Sensor_Name\").alias(\"Sensor_Name\"),\n",
    "        F.col(\"unnested_value.Hourly_Counts\").alias(\"Hourly_Counts\")\n",
    "        )\n",
    "\n",
    "# Printing final streaming dataframe schema. \n",
    "df_counts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"onesix\"></a>\n",
    "## 1.6 Transformations to prepare the columns for model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- prev_count: integer (nullable = true)\n",
      " |-- next_date: date (nullable = true)\n",
      " |-- next_Mdate: integer (nullable = true)\n",
      " |-- next_day_week: integer (nullable = true)\n",
      " |-- next_day_of_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.6\n",
    "# udf function to get week day as number from week day name as string\n",
    "get_week_day = F.udf(lambda cellVal: cellVal - 1 if cellVal > 1 else 7, T.IntegerType())\n",
    "# Getting columns not required for predictions.\n",
    "remove_cols = [c for c in df_counts.columns if c not in ['Date_Time', 'Sensor_ID', 'Time', 'Hourly_Counts']]\n",
    "\n",
    "# Filter streaming dataframe into input dataframe to prediction with required columns.\n",
    "df_filtered_counts = df_counts.\\\n",
    "    drop(*remove_cols).\\\n",
    "    withColumn('next_date', F.date_add(F.col('Date_Time'), 1)).\\\n",
    "    withColumn('next_Mdate', F.dayofmonth('next_date')).\\\n",
    "    withColumn('next_day_week', F.weekofyear('next_date')).\\\n",
    "    withColumn('next_day_of_week', get_week_day(F.dayofweek('next_date'))).\\\n",
    "    drop('Date_Time').\\\n",
    "    withColumnRenamed('Hourly_Counts', 'prev_count')\n",
    "\n",
    "# Printing filtered dataframe's schema.\n",
    "df_filtered_counts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"oneseven\"></a>\n",
    "## 1.7 Derive model predictions and saving it into parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- prev_count: integer (nullable = true)\n",
      " |-- next_date: date (nullable = true)\n",
      " |-- next_Mdate: integer (nullable = true)\n",
      " |-- next_day_week: integer (nullable = true)\n",
      " |-- next_day_of_week: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f7293e64160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.7\n",
    "# Load the model.\n",
    "model = PipelineModel.load('count_estimation_pipeline_model')\n",
    "# Get predictions from the model by passing formatted streaming dataframe.\n",
    "df_predictions = model.transform(df_filtered_counts)\n",
    "# Prinitng schema of output prediction dataframe.\n",
    "df_predictions.printSchema()\n",
    "\n",
    "# Writing streaming prediction dataframe results to file sink using parquet format.\n",
    "# With the same trigger as in Task 1. (daywise)\n",
    "df_predictions.\\\n",
    "    writeStream.\\\n",
    "    format('parquet').\\\n",
    "    outputMode(\"append\").\\\n",
    "    option(\"path\", \"parquet/prediction_stream.parquet\").\\\n",
    "    option(\"checkpointLocation\", \"parquet/prediction_stream_checkpoint\").\\\n",
    "    trigger(processingTime='5 seconds').\\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"oneeighta\"></a>\n",
    "## 1.8 (a) Counting above throshold Hourly counts from predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_date: timestamp (nullable = true)\n",
      " |-- end_date: timestamp (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- above_threshold_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.8 a\n",
    "# udf function that get count of values that are greater than 2000 in input list. \n",
    "above_threshold_count = F.udf(lambda cellVal: sum(map(lambda x: x > 2000, cellVal)), T.IntegerType())\n",
    "\n",
    "# From streaming prediction dataframe, group result by each sensor prediction on each day window.\n",
    "# Select the window frame as start and end date and display total count that exceed 2000 on that frame window. (day-window)\n",
    "df_grouped_by_threshold = df_predictions.\\\n",
    "    groupby(df_predictions.Sensor_ID, \n",
    "            F.window(df_predictions.next_date, '1 day').alias('date_window')).\\\n",
    "    agg(above_threshold_count(F.collect_list('prediction')).alias('above_threshold_count')).\\\n",
    "    select(\\\n",
    "        F.col('date_window.start').alias('start_date'), \\\n",
    "        F.col('date_window.end').alias('end_date'), \\\n",
    "        F.col('Sensor_ID'), \\\n",
    "        F.col('above_threshold_count')).\\\n",
    "    sort('date_window.start')\n",
    "\n",
    "df_grouped_by_threshold.printSchema()\n",
    "\n",
    "# Create function to show values received from input dataframe\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.\\\n",
    "    show(10, False)\n",
    "    print(f'Number of Rows processed: {df.count()}')\n",
    "\n",
    "# Write the output of performed group operation on streaming prediction dataframe to notebook\n",
    "# Using predefine foreach batch function.\n",
    "# With the same trigger as in Task 1. (daywise)\n",
    "group_by_query = df_grouped_by_threshold.\\\n",
    "    writeStream.\\\n",
    "    outputMode('complete').\\\n",
    "    foreachBatch(foreach_batch_function).\\\n",
    "    trigger(processingTime='5 seconds').\\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"oneeightb\"></a>\n",
    "## 1.8 (b) Getting above threshold for next day from prediction result and write to new Kafka topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------+---------------------+\n",
      "|start_date         |end_date           |Sensor_ID|above_threshold_count|\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|34       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|29       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|19       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|73       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|5        |34                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|68       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|63       |68                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|28       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|67       |51                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|52       |0                    |\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of Rows processed: 1860\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|start_date         |end_date           |Sensor_ID|above_threshold_count|\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|34       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|29       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|19       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|73       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|5        |36                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|68       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|63       |72                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|28       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|67       |54                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|52       |0                    |\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of Rows processed: 1860\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|start_date         |end_date           |Sensor_ID|above_threshold_count|\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|34       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|29       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|19       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|73       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|5        |36                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|68       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|63       |72                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|28       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|67       |54                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|52       |0                    |\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of Rows processed: 1860\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|start_date         |end_date           |Sensor_ID|above_threshold_count|\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|34       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|29       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|19       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|73       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|5        |36                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|68       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|63       |72                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|28       |0                    |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|67       |54                   |\n",
      "|2020-12-02 00:00:00|2020-12-03 00:00:00|52       |0                    |\n",
      "+-------------------+-------------------+---------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of Rows processed: 1860\n"
     ]
    }
   ],
   "source": [
    "#2.8 b\n",
    "\n",
    "# Geohash the location upto 1.1 m precision using 5 decimal places into latitude and longitude coordinates.\n",
    "# This key maybe used for later use, to reduce the data for almost same location points.\n",
    "def get_geohash_id(latitude_str, longitude_str):\n",
    "    return geohash.encode(float(latitude_str), float(longitude_str), 5)\n",
    "\n",
    "# Build value dictionary to send through kafka topic.\n",
    "def build_values(mdate, time, sen_id, pred, lat, lon, sn):\n",
    "    return str({'mdate': mdate, 'time': time, 'sid': sen_id, 'latitude': lat, 'longitude': lon, 'sname': sn, 'prediction': pred})\n",
    "\n",
    "# udf function to get \"key\" parameter to set in kafka topic\n",
    "get_geohash = F.udf(get_geohash_id, T.StringType())\n",
    "\n",
    "# udf function to get \"value\" parameter to set in kafka topic\n",
    "get_value = F.udf(build_values, T.StringType())\n",
    "\n",
    "# retrive all prediction results that has higher thrshold count (>2000) \n",
    "df_filtered = df_predictions.\\\n",
    "    filter(F.col('prediction')>2000)\n",
    "\n",
    "join_condition2 = [df_filtered.Sensor_ID==df_sensors.sensor_id]\n",
    "    \n",
    "\n",
    "# New sensor: 73,Bourke St - Spencer St (South) -> Not avaialble in Sensor data\n",
    "# Sensor 73 has higher pedestrain count >2000 but we do not have its location information \n",
    "# so, this sensor is exclude\n",
    "# Combine result with sensor data to retrive sensor name and location.\n",
    "df_joined = df_filtered.\\\n",
    "    join(df_sensors, join_condition2, 'inner').\\\n",
    "    select(df_sensors.sensor_id, \n",
    "           df_filtered.next_date, \n",
    "           df_filtered.Time,\n",
    "           df_filtered.prediction,\n",
    "           df_sensors.sensor_description,\n",
    "           df_sensors.latitude,df_sensors.longitude).\\\n",
    "    withColumn('next_date', F.col('next_date').cast('string')).\\\n",
    "    withColumn('geohashkey', F.lit(get_geohash('latitude', 'longitude'))).\\\n",
    "    withColumn('data', \n",
    "               F.lit(get_value(\n",
    "                   'next_date', 'Time', 'sensor_id', 'prediction',\n",
    "                   'latitude', 'longitude', 'sensor_description'))).\\\n",
    "    select('geohashkey', 'data')\n",
    "\n",
    "# Declare new topic name to create new kafka topic that holds prediction results.\n",
    "new_topic_name = f'{topic}_Prediction_Location'\n",
    "\n",
    "# Create new kapka stream with key value retrived from join streaming quey.\n",
    "# Alos, we only sending data that is new or updated using output mode update.\n",
    "join_query = df_joined.\\\n",
    "    selectExpr(\"CAST(geohashkey AS STRING) as key\", \"CAST(data AS STRING) as value\").\\\n",
    "    writeStream.\\\n",
    "    format(\"kafka\").\\\n",
    "    option(\"kafka.bootstrap.servers\",\"localhost:9092\").\\\n",
    "    option(\"topic\", new_topic_name).\\\n",
    "    option(\"checkpointLocation\", f'{new_topic_name}_checkpoint').\\\n",
    "    outputMode(\"update\").\\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
